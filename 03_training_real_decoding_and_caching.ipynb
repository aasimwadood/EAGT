{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e66f69",
   "metadata": {},
   "source": [
    "# EAGT Training â€” Real Frame/Audio Decoding + Dataset Caching (All-in-One)\n",
    "\n",
    "This notebook provides a **practical training pipeline** for the Emotion-Aware Generative AI Tutor (EAGT):\n",
    "1. **CSV ingestion** compatible with DAiSEE/SEMAINE-style manifests.\n",
    "2. **Robust video frame decoding** using OpenCV (cv2) with clip sampling.\n",
    "3. **Audio decoding** with `soundfile`/`librosa`, or **FFmpeg** fallback extraction from video.\n",
    "4. **Feature transforms** (resize/normalize for vision, Mel-spectrograms for audio).\n",
    "5. **Disk caching** of preprocessed tensors to accelerate subsequent epochs.\n",
    "6. **PyTorch dataset + dataloaders** with multi-worker I/O.\n",
    "7. **Toy fusion model** (vision CNN + audio CNN) and a training loop scaffold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9922bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python soundfile librosa torch torchvision torchaudio matplotlib scikit-learn pyyaml\n",
    "# !apt-get -y install ffmpeg\n",
    "import os, sys, math, time, json, csv, hashlib, subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "SEED=1337\n",
    "np.random.seed(SEED); torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f582675",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a993d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    csv_train: str = 'configs/daisee_train.csv'\n",
    "    csv_val:   str = 'configs/daisee_val.csv'\n",
    "    cache_dir: str = '.cache_eagt'\n",
    "    num_workers: int = 4\n",
    "    batch_size: int = 8\n",
    "    epochs: int = 2\n",
    "    lr: float = 3e-4\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    clip_seconds: float = 1.0\n",
    "    fps: int = 8\n",
    "    img_size: int = 112\n",
    "    sr: int = 16000\n",
    "    n_mels: int = 64\n",
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d92eb",
   "metadata": {},
   "source": [
    "## 2. FFmpeg Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31373c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_ffmpeg():\n",
    "    try:\n",
    "        subprocess.run(['ffmpeg','-version'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "def extract_audio_ffmpeg(video_path: str, out_wav: str, sr: int=16000, overwrite=False) -> bool:\n",
    "    Path(out_wav).parent.mkdir(parents=True, exist_ok=True)\n",
    "    cmd = ['ffmpeg', '-y' if overwrite else '-n', '-i', video_path, '-ac','1','-ar',str(sr),'-vn','-loglevel','error', out_wav]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21054564",
   "metadata": {},
   "source": [
    "## 3. Video Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be38fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_clip(path: str, clip_seconds: float, fps: int, img_size: int) -> np.ndarray:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f'Cannot open video: {path}')\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    src_fps = cap.get(cv2.CAP_PROP_FPS) or fps\n",
    "    duration = total_frames / max(src_fps, 1)\n",
    "    center_t = duration/2.0\n",
    "    needed = int(clip_seconds*fps)\n",
    "    times = np.linspace(center_t - clip_seconds/2, center_t + clip_seconds/2, num=needed, endpoint=False)\n",
    "    frames = []\n",
    "    for t in times:\n",
    "        idx = int(t * src_fps)\n",
    "        idx = np.clip(idx, 0, max(total_frames-1,0))\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            frame = np.zeros((img_size, img_size,3), dtype=np.uint8)\n",
    "        else:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (img_size, img_size), interpolation=cv2.INTER_AREA)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    arr = np.stack(frames, axis=0).astype(np.float32)/255.0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761026c",
   "metadata": {},
   "source": [
    "## 4. Audio Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81975859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio_clip(audio_path: str, video_path: str, clip_seconds: float, sr: int, cache_root: Path) -> np.ndarray:\n",
    "    wav = None; path=None\n",
    "    if audio_path and Path(audio_path).exists(): path=audio_path\n",
    "    else:\n",
    "        if have_ffmpeg() and Path(video_path).exists():\n",
    "            import hashlib\n",
    "            vid_hash = hashlib.md5(video_path.encode('utf-8')).hexdigest()[:10]\n",
    "            out_wav = cache_root/ 'extracted_audio' / f'{vid_hash}.wav'\n",
    "            if not out_wav.exists(): extract_audio_ffmpeg(video_path, str(out_wav), sr=sr, overwrite=False)\n",
    "            if out_wav.exists(): path=str(out_wav)\n",
    "    if path and Path(path).exists():\n",
    "        try:\n",
    "            wav, srr = sf.read(path, dtype='float32')\n",
    "            if wav.ndim==2: wav = wav.mean(-1)\n",
    "            if srr!=sr: wav = librosa.resample(wav, orig_sr=srr, target_sr=sr)\n",
    "        except Exception:\n",
    "            wav=None\n",
    "    if wav is None: wav = np.zeros(int(sr*clip_seconds), dtype=np.float32)\n",
    "    target = int(sr*clip_seconds)\n",
    "    if len(wav)>target:\n",
    "        s=(len(wav)-target)//2; wav=wav[s:s+target]\n",
    "    elif len(wav)<target:\n",
    "        pad=target-len(wav); wav=np.pad(wav,(pad//2,pad-pad//2))\n",
    "    return wav.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a186363",
   "metadata": {},
   "source": [
    "## 5. Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d00831",
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_MEAN=np.array([0.485,0.456,0.406],dtype=np.float32); IM_STD=np.array([0.229,0.224,0.225],dtype=np.float32)\n",
    "def vision_transform(frames: np.ndarray)->torch.Tensor:\n",
    "    x=(frames-IM_MEAN)/IM_STD\n",
    "    x=np.transpose(x,(3,0,1,2))\n",
    "    return torch.from_numpy(x.astype(np.float32))\n",
    "def audio_mel_transform(wav: np.ndarray, sr:int, n_mels:int)->torch.Tensor:\n",
    "    mel=librosa.feature.melspectrogram(y=wav,sr=sr,n_mels=n_mels,fmin=30,fmax=sr//2)\n",
    "    mel=np.log(np.maximum(1e-8,mel)).astype(np.float32)\n",
    "    m,s=mel.mean(),mel.std()+1e-6; mel=(mel-m)/s\n",
    "    return torch.from_numpy(mel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b21c34",
   "metadata": {},
   "source": [
    "## 6. Cache & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, hashlib\n",
    "def cache_key(video_path:str,audio_path:str,label:str,cfg)->str:\n",
    "    h=hashlib.md5(); h.update(video_path.encode()); h.update((audio_path or '').encode()); h.update(label.encode())\n",
    "    blob=json.dumps({'clip_seconds':cfg.clip_seconds,'fps':cfg.fps,'img_size':cfg.img_size,'sr':cfg.sr,'n_mels':cfg.n_mels},sort_keys=True)\n",
    "    h.update(blob.encode()); return h.hexdigest()\n",
    "def cache_path(root:Path,key:str)->Path: return root/f'{key}.pt'\n",
    "\n",
    "LABELS=['frustration','confusion','boredom','engagement']; L2I={l:i for i,l in enumerate(LABELS)}\n",
    "class VideoAudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,csv_path:str,cfg):\n",
    "        self.df=pd.read_csv(csv_path); self.cfg=cfg; self.root=Path(cfg.cache_dir); self.root.mkdir(parents=True,exist_ok=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self,idx:int):\n",
    "        r=self.df.iloc[idx]\n",
    "        v=str(r['video_path']); a=str(r['audio_path']) if 'audio_path' in r and not pd.isna(r['audio_path']) else ''\n",
    "        lab=str(r['label']).lower().strip(); y=L2I.get(lab,0)\n",
    "        key=cache_key(v,a,lab,self.cfg); c=cache_path(self.root,key)\n",
    "        if c.exists():\n",
    "            blob=torch.load(c); return blob['vision'],blob['audio'],y\n",
    "        frames=read_video_clip(v,self.cfg.clip_seconds,self.cfg.fps,self.cfg.img_size)\n",
    "        wav=read_audio_clip(a,v,self.cfg.clip_seconds,self.cfg.sr,self.root)\n",
    "        vis=vision_transform(frames); mel=audio_mel_transform(wav,self.cfg.sr,self.cfg.n_mels)\n",
    "        torch.save({'vision':vis,'audio':mel}, c)\n",
    "        return vis,mel,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa5d13",
   "metadata": {},
   "source": [
    "## 7. Model (Toy Fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17110d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTiny(nn.Module):\n",
    "    def __init__(self,out_dim=128):\n",
    "        super().__init__(); self.net=nn.Sequential(nn.Conv2d(3*CFG.fps,64,3,2,1),nn.ReLU(),nn.Conv2d(64,128,3,2,1),nn.ReLU(),nn.AdaptiveAvgPool2d(1),nn.Flatten(),nn.Linear(128,out_dim))\n",
    "    def forward(self,x): b,c,t,h,w=x.shape; x=x.reshape(b,c*t,h,w); return self.net(x)\n",
    "class AudioTiny(nn.Module):\n",
    "    def __init__(self,out_dim=128):\n",
    "        super().__init__(); self.net=nn.Sequential(nn.Conv2d(1,32,5,2,2),nn.ReLU(),nn.Conv2d(32,64,5,2,2),nn.ReLU(),nn.AdaptiveAvgPool2d(1),nn.Flatten(),nn.Linear(64,out_dim))\n",
    "    def forward(self,mel): return self.net(mel.unsqueeze(1))\n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self,feat=128,nc=len(LABELS)):\n",
    "        super().__init__(); self.v=VisionTiny(feat); self.a=AudioTiny(feat); self.cls=nn.Sequential(nn.ReLU(),nn.Dropout(0.3),nn.Linear(2*feat,nc))\n",
    "    def forward(self,vis,mel): return self.cls(torch.cat([self.v(vis),self.a(mel)],-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6711809e",
   "metadata": {},
   "source": [
    "## 8. Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2030820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(cfg: Config):\n",
    "    tr=VideoAudioDataset(cfg.csv_train,cfg); va=VideoAudioDataset(cfg.csv_val,cfg)\n",
    "    dl_tr=DataLoader(tr,batch_size=cfg.batch_size,shuffle=True,num_workers=cfg.num_workers,pin_memory=True,persistent_workers=cfg.num_workers>0)\n",
    "    dl_va=DataLoader(va,batch_size=cfg.batch_size,shuffle=False,num_workers=cfg.num_workers,pin_memory=True,persistent_workers=cfg.num_workers>0)\n",
    "    return dl_tr, dl_va"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae0e7a",
   "metadata": {},
   "source": [
    "## 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, opt, device, train=True):\n",
    "    model.train(train); tot=0; n=0; corr=0\n",
    "    for vis,mel,y in loader:\n",
    "        vis,mel,y=vis.to(device),mel.to(device),y.to(device)\n",
    "        if train: opt.zero_grad()\n",
    "        logits=model(vis,mel); loss=nn.CrossEntropyLoss()(logits,y)\n",
    "        if train: loss.backward(); opt.step()\n",
    "        pred=logits.argmax(-1); corr+=(pred==y).sum().item(); n+=y.numel(); tot+=loss.item()*y.numel()\n",
    "    return tot/max(n,1), corr/max(n,1)\n",
    "\n",
    "def validate_report(model, loader, device):\n",
    "    y_true=[]; y_pred=[]; model.eval()\n",
    "    with torch.no_grad():\n",
    "        for vis,mel,y in loader:\n",
    "            logits=model(vis.to(device),mel.to(device)); y_pred+=logits.argmax(-1).cpu().tolist(); y_true+=y.tolist()\n",
    "    print(classification_report(y_true,y_pred,target_names=LABELS,digits=3))\n",
    "\n",
    "def train_main(cfg: Config):\n",
    "    dl_tr, dl_va = make_loaders(cfg)\n",
    "    model=FusionClassifier().to(cfg.device); opt=optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "    for ep in range(1,cfg.epochs+1):\n",
    "        tr_loss,tr_acc=run_epoch(model,dl_tr,opt,cfg.device,train=True)\n",
    "        va_loss,va_acc=run_epoch(model,dl_va,opt,cfg.device,train=False)\n",
    "        print(f'Epoch {ep:02d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f}')\n",
    "    print('\\nValidation Report:'); validate_report(model,dl_va,cfg.device); return model\n",
    "\n",
    "# model = train_main(CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12e908",
   "metadata": {},
   "source": [
    "## 10. Visual Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b87664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(ds: VideoAudioDataset, idx=0):\n",
    "    vis, mel, y = ds[idx]\n",
    "    c,t,h,w = vis.shape\n",
    "    cols=min(t,8); rows=int(np.ceil(t/cols))\n",
    "    fig,ax=plt.subplots(rows,cols,figsize=(cols*1.5,rows*1.5)); ax=np.array(ax).reshape(rows,cols)\n",
    "    k=0\n",
    "    for r in range(rows):\n",
    "        for c0 in range(cols):\n",
    "            a=ax[r,c0]; a.axis('off')\n",
    "            if k<t:\n",
    "                img=vis[:,k].permute(1,2,0).numpy(); img=(img*IM_STD+IM_MEAN).clip(0,1); a.imshow(img)\n",
    "            k+=1\n",
    "    plt.suptitle(f'Label: {LABELS[y]} (frames)'); plt.show()\n",
    "    plt.figure(figsize=(6,3)); plt.imshow(mel.numpy(),aspect='auto',origin='lower'); plt.colorbar(); plt.title('Mel (norm)'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ds = VideoAudioDataset(CFG.csv_train, CFG); show_sample(ds,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
